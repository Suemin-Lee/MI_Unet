{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    " \n",
    "from tqdm import tqdm \n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "TRAIN_PATH = 'stage1_train/'\n",
    "TEST_PATH = 'stage1_test/'\n",
    "\n",
    "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
    "test_ids = next(os.walk(TEST_PATH))[1]\n",
    "\n",
    "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "Y_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "\n",
    "# resizing the input data into 128*128 image pixel\n",
    "\n",
    "print('Resizing training images and masks')\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n",
    "    path = TRAIN_PATH + id_\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]  \n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_train[n] = img  #Fill empty X_train with values from img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "        mask_ = imread(path + '/masks/' + mask_file)\n",
    "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n",
    "                                      preserve_range=True), axis=-1)\n",
    "        mask = np.maximum(mask, mask_)  \n",
    "            \n",
    "    Y_train[n] = mask   \n",
    "\n",
    "# test images\n",
    "sizes_test = []\n",
    "print('Resizing test images and masks for testing') \n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = TEST_PATH + id_\n",
    "    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
    "    sizes_test.append([img.shape[0], img.shape[1]])\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_test[n] = img\n",
    "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "        mask_ = imread(path + '/masks/' + mask_file)\n",
    "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n",
    "                                      preserve_range=True), axis=-1)\n",
    "        mask = np.maximum(mask, mask_)  \n",
    "            \n",
    "    Y_test[n] = mask  \n",
    "\n",
    "print('Resizing images... Done!')\n",
    "\n",
    "# Testing to see the input images\n",
    "image_x = random.randint(0, len(train_ids))\n",
    "imshow(X_train[image_x])\n",
    "plt.show()\n",
    "imshow(np.squeeze(Y_train[image_x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dice_coef\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred, smooth=1):\n",
    "    return -dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "# Full U-net model functions\n",
    "\n",
    "def build_model(input_layer, start_neurons):\n",
    "    s = keras.layers.Lambda(lambda x: x / 255)(input_layer)\n",
    "\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(s)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "#     pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    \n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n",
    "\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (2, 2), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    \n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (2, 2), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    \n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (2, 2), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (2, 2), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    \n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "input_layer1  = keras.layers.Input(shape=(128,128,3))\n",
    "output_layer = build_model(input_layer1, 16)\n",
    "\n",
    "model = Model(input = input_layer1, output = output_layer)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(lr=0.001),\n",
    "              metrics=[dice_coef])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "nb_classes =2                   # Binary classifications\n",
    "Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
    "\n",
    "y_train = Y_train               #not necessary but to match up with the previous MINST code \n",
    "y_test = Y_test                 #not necessary but to match up with the previous MINST code \n",
    "\n",
    "trn = Dataset(X_train, Y_train, y_train, nb_classes)\n",
    "tst = Dataset(X_test , Y_test, y_test, nb_classes)\n",
    "\n",
    "cfg = {}\n",
    "cfg['SGD_BATCHSIZE']    = 16\n",
    "cfg['SGD_LEARNINGRATE'] = 0.001\n",
    "cfg['NUM_EPOCHS']       = 3    # For testing >> usual 5,000 epochs or more is appropriate\n",
    "cfg['FULL_MI']          = True\n",
    "\n",
    "\n",
    "cfg['ACTIVATION'] = 'relu'   #for U-net\n",
    "# cfg['ACTIVATION'] = 'tanh'\n",
    "\n",
    "# How many hidden neurons to put into each of the layers\n",
    "cfg['LAYER_DIMS'] = [128,64,32,16,8,16,32,64,10000] \n",
    "ARCH_NAME =  '-'.join(map(str,cfg['LAYER_DIMS']))\n",
    "\n",
    "# Where to save activation and weights data\n",
    "cfg['SAVE_DIR'] = 'rawdata/' + cfg['ACTIVATION'] + '_' + ARCH_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loggingreporter            # Saving activations \n",
    "\n",
    "def do_report(epoch):\n",
    "    # Only log activity for some epochs.  Mainly this is to make things run faster.\n",
    "    if epoch < 20:       # Log for all first 20 epochs\n",
    "        return True\n",
    "    elif epoch < 100:    # Then for every 5th epoch\n",
    "        return (epoch % 5 == 0)\n",
    "    elif epoch < 200:    # Then every 10th\n",
    "        return (epoch % 10 == 0)\n",
    "    else:                # Then every 100th\n",
    "        return (epoch % 100 == 0)\n",
    "\n",
    "    \n",
    "reporter = loggingreporter.LoggingReporter(cfg=cfg, \n",
    "                                          trn=trn, \n",
    "                                          tst=tst, \n",
    "                                          do_save_func=do_report)\n",
    "\n",
    "\n",
    "r = model.fit(X_train, Y_train, verbose  = 2, \n",
    "              batch_size = 16, epochs = cfg['NUM_EPOCHS'] ,\n",
    "              validation_data=(tst.X, tst.Y),\n",
    "              callbacks  = [reporter,])\n",
    "#                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
